50+6
x <- 945
x
x = 5
y <- 3
4 -> z
z
ls
ls()
rm(x)
ls()
x <- c(5,2,7,4)
x
z <- c(T,F,T)
z
w <- c("bla", "zdravo, "hojmene")
w <- c("bla","zdravo,"hojmene")
w<-c("bla","zdravo,"hojmene")
w<-c("bla","zdravo","hojmene")
w
v <- 1:10
v
rm(ls())
rm(list=ls())
ls()
v <- seq(from=5,to=50, by=3)
v
x <- 522
a <- c(v,x, 5.3)
a
rm(list=ls())
x <- 1:4
y <- c(10,20,30,40)
x+y
x+1
v <- c(2,3)
x+v
v+y
x <- c(10,-20,30,-40,50)
x > 0
sqrt(c(1,4,9,16))
lenght(x)
length(x)
sort(x)
x[2]
x[c(1,3)]
x[1:3]
x[-1]
x[c(-1,-)]
x[c(-1,-4)]
x[x > 0]
x[x < 0 & x > -40]
x[2] <- -25
x
x[c(4,5)] <- c(-45,-55)
x
x[x<0] <- 0
x
x[x<0] <- -20
x
x[x<1] <- -20
x
x[x<0] <- c(0,1)
x[x<0] <- c(0,1,3)
x
rm(x[x<2])
x <- x[x<2]
x
x <- c(10,-20,30,-40,50)
x
x[g
]
x[]
x <-5
x
x <- c(10,-20,30,-40,50)
x[] <- 5
x
student <- list(id=12345,name="Marko",marks=c(10,9,10,9,8,10))
student
clear
clear()
name <- c("Tom", "Mary", "Anna")
age <- c(51,47,19)
gender <- c("m", "f", "f")
student <- c(F,F,T)
data.frame(name,age,gender,student)
df <-data.frame(name,age,gender,student)
df
df[3,2]
df[3]
df[c(1,2),3]
df[1,c(1,2)]
df[1,c(1,3)]
df[-1, c(1,3)]
df[,1]
df[2,2]
df[2,1]
df[,"name"]
df$name
df$name,age
q()
v <- 1:20
v
k <- 20:1
k
v <- c(v,k)
v
k <- 19:1
v <- 1:20
v <- c(v,k)
v <- c(v,k)
k <- 19:1
v <- 1:20
v <- c(v,k)
v
clear
v <- c(1,3,5)
k <- rep(v, times = 10)
k
ls(9
)
ls()
rm(list = ls())
ls()
gender <- c("f","m","m","m","f","m","f")
gender
gender <- factor(gender)
gender
smeri <- factor(c('levo','levo','desno'), levels = c('levo','desno','gor','dol'))
smeri
tocke <- seq(start=0, to=1, step=0.1)
tocke <- seq(from=0, to=1, by=0.1)
tockež
tocke
x <- 5
sin(x)
sin(tocke)
height <- c(179, 185, 183, 172, 174, 185, 193, 169, 173, 168)
weight <- c(95, 89, 70, 80, 92, 86, 100, 63, 72, 70)
bmi <- wiight / (height / 100)^2
bmi <- weight / (height / 100)^2
bmi
lenght(bmi)
length(bmi)
x <- c(1, -2, 3, -4, 5, -6, 7, -8)
x[x > 0]
x[x > 0] * 10
x <- c(1,2,3)
x[1]/x[2]^2-1+2*x[3]-x[1+1]
x <- 1:200
length(x[x %% 11 == 0])
q()
gcd(5,8)
q()
getwd()
install.packages(c("tm", "SnowballC", "wordcloud", "proxy", "kernlab", "NLP", "openNLP")) 
install.packages("openNLPmodels.en", repos="http://datacube.wu.ac.at/")
conn = file("time.mag.txt", open="r")
text = readLines(conn)
close(conn)
q()
length(text)
head(text)
library(tm)
corpus <- Corpus(VectorSource(text))
length(corpus)
content(corpus[[1]])
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
content(corpus[[1]])
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
conn = file("english.stop.txt", open="r")
mystopwords = readLines(conn)
close(conn)
# Izpisimo ta seznam besed
mystopwords
corpus <- tm_map(corpus, removeWords, mystopwords)
# Izvedemo postopek krnjenja besed v vseh dokumentih korpusa.
# Krnjenje odreze koncnico besede tako, da ostane le njen koren 
# Na ta nacin zdruzimo pomensko sorodne, a razlicne besede v enotno obliko
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
# Poglejmo vsebino prvega dokumenta v predelanem korpusu
content(corpus[[1]])
# Matrika pojavitev beseda-dokument opisuje pogostost pojavljanja besed v dokumentih.
# Vrstice matrike ustrezajo posameznim besedam, stolpci pa dokumentom v korpusu.
# Vrednost elementa (i,j) doloca pogostost pojavljanja i-te besede v j-tem dokumentu korpusa. 
# Matrika pojavitev beseda-dokument opisuje pogostost pojavljanja besed v dokumentih.
# Vrstice matrike ustrezajo posameznim besedam, stolpci pa dokumentom v korpusu.
# Vrednost elementa (i,j) doloca pogostost pojavljanja i-te besede v j-tem dokumentu korpusa. 
tdm <- TermDocumentMatrix(corpus)
# V nasem primeru matrika pojavitev beseda-dokument vsebuje 14557 besed in 425 dokumentov. 
# Matrika ima zelo malo nenicelnih elementov (99% elementov so nicle)
tdm
# Izpisimo seznam besed v korpusu
rownames(tdm)
idx <- which(rownames(tdm) == "moscow")
idx
# Poglejmo pogostost pojavljanja besede "moscow" v dokumentih iz korpusa
inspect(tdm[idx,])
# Poiscimo pogoste besede (v konkretnem primeru take, ki se pojavijo najmanj 300 krat v celotnem korpusu)
findFreqTerms(tdm, lowfreq=300)
# Ponazorimo frekvenco pojavitev najpogostejsih besed v korpusu
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency >= 300)
barplot(termFrequency)
library(wordcloud)
# Pogostost pojavljanja besed v korpusu dolocimo s sestevanjem elementov posameznih vrstic matrike pojavitev beseda-dokument
mat <- as.matrix(tdm)
wordFreq <- sort(rowSums(mat), decreasing=TRUE)
grayLevels <- gray((wordFreq+10) / (max(wordFreq)+10))
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=100, random.order=F, colors=grayLevels)
findAssocs(tdm, "moscow", 0.5)
# Odstranimo redke besede (v konkretnem primeru tiste, ki se ne pojavljajo vsaj v 70% dokumentov)
tdm2 <- removeSparseTerms(tdm, sparse=0.7)
mat <- as.matrix(tdm2)
# Razdalje med besedami lahko izracunamo z uporabo funkcije dist()
distMatrix <- dist(mat)
tdm2
# S pomocjo hierarhicnega razvrscanja lahko identificiramo skupine besed, ki pogosto nastopajo skupaj v dokumentih
fit <- hclust(distMatrix, method="ward.D")
# Hierarhicno zgradbo slikovno ponazorimo z dendrogramom. Listi drevesa predstavljajo posamezne besede, 
# notranja vozlisca pa zdruzene podskupine besed
plot(fit)
# Zgradimo matriko pojavitev dokument-beseda. Vrstice matrike ustrezajo dokumentom v korpusu, stolpci pa besedam.
# Vrednost elementa (i,j) opisuje pojavitev j-te besede v i-tem dokumentu (v tem primeru uporabimo Tf-Idf utezevanje)
dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
mat <- as.matrix(dtm)
# Razvrscanje dokumentov z uporabo metode kmeans (v konkretnem primeru dokumente razvrscamo v 3 skupine) 
k <- 3
kmeansResult <- kmeans(mat, k)
# Izpisimo najpogostejse besede iz vsake skupine dokumentov.
# Ta izpis nam pomaga pri avtomatskem dolocanju teme dokumentov.
for (i in 1:k) 
{
s <- sort(kmeansResult$centers[i,], decreasing=T)
cat(names(s)[1:10], "\n")
}
#
# Podobnost med dokumenti
#
dtm
mat
dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
mat <- as.matrix(dtm)
# Razvrscanje dokumentov z uporabo metode kmeans (v konkretnem primeru dokumente razvrscamo v 3 skupine) 
k <- 3
kmeansResult <- kmeans(mat, k)
kmeansResult
for (i in 1:k) 
{
s <- sort(kmeansResult$centers[i,], decreasing=T)
cat(names(s)[1:10], "\n")
}
corpus <-Corpus(DirSource("economy"))
length(corpus )
#
# Podobnost med dokumenti
#
# Izracunajmo, koliko se dokumenti razlikujejo med seboj. Podobnost med dokumenti lahko izracunamo s pomocjo kosinusne razdalje.
# Dokumente predstavimo kot vektorje: besede predstavljajo dimenzije vektorskega prostora, frekvence besed v dokumentu pa 
# razseznosti v teh dimenzijah. Podobnost med dokumenti izrazimo kot razdaljo v vektorskem prostoru. Kosinusna razdalja oceni
# podobnost med dokumenti kot kosinus kota med vektorskimi predstavitvami dokumentov.
library(proxy)
# Izracunajmo razdaljo med prvim in drugim dokumentom v korpusu
content(corpus[[1]])
content(corpus[[2]])
dist(as.matrix(dtm)[c(1,2),], method = "cosine")
# Poiscimo najbolj podobna dokumenta v korpusu...
dtm2 <- removeSparseTerms(dtm, sparse=0.8)
mat <- as.matrix(dtm2)
dist.mat <- as.matrix(dist(mat, method = "cosine"))
sim.idx <- which(dist.mat == min(dist.mat[dist.mat > 0]), arr.ind = TRUE)
sim.idx
# Poglejmo najbolj podobna dokumenta
content(corpus[[sim.idx[1,1]]])
content(corpus[[sim.idx[1,2]]])
dist(mat[c(sim.idx[1,1], sim.idx[1,2]),], method = "cosine")
corpus <-Corpus(DirSource("economy"))
length(corpus )
corpus  <- tm_map(corpus , content_transformer(tolower))
corpus  <- tm_map(corpus , content_transformer(tolower))
corpus  <- tm_map(corpus , removeWords, stopwords('english'))
corpus  <- tm_map(corpus , removeNumbers)
corpus  <- tm_map(corpus , removePunctuation)
corpus  <- tm_map(corpus , stemDocument)
corpus  <- tm_map(corpus , stripWhitespace)
#
# Preoblikovanje korpusa v ucno mnozico
#
# Zgradimo matriko pojavitev dokument-beseda
data.tfidf <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
data.tfidf <- removeSparseTerms(data.tfidf, 0.95)
# Preberemo podatke o tematiki dokumentov, ki sluzijo kot razredi v klasifikaciji
Topic <-read.table("economy-topics.txt")
# Zdradimo ucno mnozico tako, da matriki pojavitev dokument-beseda dodamo stolpec s tematiko dokumentov (razred)
data <- cbind(as.matrix(data.tfidf), Topic)
names(data)[ncol(data)] <- "Topic"
# Ucno mnozico razdelimo na dejansko ucno mnozico in testno mnozico
set.seed(75785678)
sel <- sample(nrow(data), 200, F)
train <- data[-sel,]
test <- data[sel,]
#
# Klasifikacija dokumentov z modelom KNN
#
summary(data)
library(class)
# Poiscemo stolpec, ki predstavlja razred
r <- which(names(data)=="Topic")
# Uporabimo model KNN za dolocitev tematike dokumentov v testni mnozici
predicted <- knn(train[,-r], test[,-r], train$Topic)
observed <- test$Topic
t <- table(observed, predicted)
t
# Klasifikacijska tocnost
sum(diag(t))/sum(t)
# Priklic (recall) predstavlja delez pravilno klasificiranih pomembnih dokumentov med vsemi pomembnimi dokumenti v korpusu (v konkretnem primeru glede na razred "general")
t[1,1]/sum(t[1,])
t[1,1]/sum(t[,1])
# Priklic (recall) predstavlja delez pravilno klasificiranih pomembnih dokumentov med vsemi pomembnimi dokumenti v korpusu (v konkretnem primeru glede na razred "general")
t[1,1]/sum(t[1,])
# Preciznost (precision) predstavlja delez pravilno klasificiranih dokumentov, ki so bili klasificirani kot pomembni (v konkretnem primeru glede na razred "general") 
t[1,1]/sum(t[,1])
library(kernlab)
model.svm <- ksvm(Topic ~ ., train, kernel = "rbfdot")
predicted <- predict(model.svm, test, type = "response")
t <- table(observed, predicted)
t
# Klasifikacijska tocnost
sum(diag(t))/sum(t)
# Priklic (v konkretnem primeru glede na razred "general")
t[1,1]/sum(t[1,])
# Preciznost (v konkretnem primeru glede na razred "general")
t[1,1]/sum(t[,1])
# SVM z radialno bazno jedrno funkcijo
model.svm <- ksvm(Topic ~ ., train, kernel = "rbfdot")
predicted <- predict(model.svm, test, type = "response")
t <- table(observed, predicted)
t
# Klasifikacijska tocnost
sum(diag(t))/sum(t)
# Priklic (v konkretnem primeru glede na razred "general")
t[1,1]/sum(t[1,])
# Preciznost (v konkretnem primeru glede na razred "general")
t[1,1]/sum(t[,1])
# Predobdelava kopusa
corpus  <- tm_map(corpus , content_transformer(tolower))
corpus  <- tm_map(corpus , removeWords, stopwords('english'))
corpus  <- tm_map(corpus , removeNumbers)
corpus  <- tm_map(corpus , removePunctuation)
corpus  <- tm_map(corpus , stemDocument)
corpus  <- tm_map(corpus , stripWhitespace)
#
# Preoblikovanje korpusa v ucno mnozico
#
# Zgradimo matriko pojavitev dokument-beseda
data.tfidf <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
data.tfidf <- removeSparseTerms(data.tfidf, 0.95)
# Preberemo podatke o tematiki dokumentov, ki sluzijo kot razredi v klasifikaciji
Topic <-read.table("economy-topics.txt")
# Zdradimo ucno mnozico tako, da matriki pojavitev dokument-beseda dodamo stolpec s tematiko dokumentov (razred)
data <- cbind(as.matrix(data.tfidf), Topic)
names(data)[ncol(data)] <- "Topic"
# Ucno mnozico razdelimo na dejansko ucno mnozico in testno mnozico
set.seed(75785678)
sel <- sample(nrow(data), 200, F)
train <- data[-sel,]
test <- data[sel,]
#
# Klasifikacija dokumentov z modelom KNN
#
library(class)
# Poiscemo stolpec, ki predstavlja razred
r <- which(names(data)=="Topic")
# Uporabimo model KNN za dolocitev tematike dokumentov v testni mnozici
predicted <- knn(train[,-r], test[,-r], train$Topic)
observed <- test$Topic
t <- table(observed, predicted)
t
# Klasifikacijska tocnost
sum(diag(t))/sum(t)
# Priklic (recall) predstavlja delez pravilno klasificiranih pomembnih dokumentov med vsemi pomembnimi dokumenti v korpusu (v konkretnem primeru glede na razred "general")
t[1,1]/sum(t[1,])
# Preciznost (precision) predstavlja delez pravilno klasificiranih dokumentov, ki so bili klasificirani kot pomembni (v konkretnem primeru glede na razred "general") 
t[1,1]/sum(t[,1])
#
# Klasifikacija dokumentov z modelom SVM
#
library(kernlab)
# SVM z radialno bazno jedrno funkcijo
model.svm <- ksvm(Topic ~ ., train, kernel = "rbfdot")
predicted <- predict(model.svm, test, type = "response")
t <- table(observed, predicted)
t
# Klasifikacijska tocnost
sum(diag(t))/sum(t)
# Priklic (v konkretnem primeru glede na razred "general")
t[1,1]/sum(t[1,])
# Preciznost (v konkretnem primeru glede na razred "general")
t[1,1]/sum(t[,1])
library(NLP)
library(openNLP)
library(NLP)
library(openNLP)
install.packages(c("tm", "SnowballC", "wordcloud", "proxy", "kernlab", "NLP", "openNLP")) 
library(NLP)
library(openNLP)
library(openNLPmodels.en)
s <- "Steven Allan Spielberg is an American filmmaker and business magnate. Spielberg is consistently considered as one of the leading pioneers of the New Hollywood era, as well as being viewed as one of the most popular and influential filmmakers in the history of cinema."
s <- as.String(s)
# zaznavanje stavkov 
sent_ann <- Maxent_Sent_Token_Annotator()
library(openNLP)
library(openNLP)
q()
q()
