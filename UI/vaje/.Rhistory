50+6
x <- 945
x
x = 5
y <- 3
4 -> z
z
ls
ls()
rm(x)
ls()
x <- c(5,2,7,4)
x
z <- c(T,F,T)
z
w <- c("bla", "zdravo, "hojmene")
w <- c("bla","zdravo,"hojmene")
w<-c("bla","zdravo,"hojmene")
w<-c("bla","zdravo","hojmene")
w
v <- 1:10
v
rm(ls())
rm(list=ls())
ls()
v <- seq(from=5,to=50, by=3)
v
x <- 522
a <- c(v,x, 5.3)
a
rm(list=ls())
x <- 1:4
y <- c(10,20,30,40)
x+y
x+1
v <- c(2,3)
x+v
v+y
x <- c(10,-20,30,-40,50)
x > 0
sqrt(c(1,4,9,16))
lenght(x)
length(x)
sort(x)
x[2]
x[c(1,3)]
x[1:3]
x[-1]
x[c(-1,-)]
x[c(-1,-4)]
x[x > 0]
x[x < 0 & x > -40]
x[2] <- -25
x
x[c(4,5)] <- c(-45,-55)
x
x[x<0] <- 0
x
x[x<0] <- -20
x
x[x<1] <- -20
x
x[x<0] <- c(0,1)
x[x<0] <- c(0,1,3)
x
rm(x[x<2])
x <- x[x<2]
x
x <- c(10,-20,30,-40,50)
x
x[g
]
x[]
x <-5
x
x <- c(10,-20,30,-40,50)
x[] <- 5
x
student <- list(id=12345,name="Marko",marks=c(10,9,10,9,8,10))
student
clear
clear()
name <- c("Tom", "Mary", "Anna")
age <- c(51,47,19)
gender <- c("m", "f", "f")
student <- c(F,F,T)
data.frame(name,age,gender,student)
df <-data.frame(name,age,gender,student)
df
df[3,2]
df[3]
df[c(1,2),3]
df[1,c(1,2)]
df[1,c(1,3)]
df[-1, c(1,3)]
df[,1]
df[2,2]
df[2,1]
df[,"name"]
df$name
df$name,age
q()
v <- 1:20
v
k <- 20:1
k
v <- c(v,k)
v
k <- 19:1
v <- 1:20
v <- c(v,k)
v <- c(v,k)
k <- 19:1
v <- 1:20
v <- c(v,k)
v
clear
v <- c(1,3,5)
k <- rep(v, times = 10)
k
ls(9
)
ls()
rm(list = ls())
ls()
gender <- c("f","m","m","m","f","m","f")
gender
gender <- factor(gender)
gender
smeri <- factor(c('levo','levo','desno'), levels = c('levo','desno','gor','dol'))
smeri
tocke <- seq(start=0, to=1, step=0.1)
tocke <- seq(from=0, to=1, by=0.1)
tocke
tocke
x <- 5
sin(x)
sin(tocke)
height <- c(179, 185, 183, 172, 174, 185, 193, 169, 173, 168)
weight <- c(95, 89, 70, 80, 92, 86, 100, 63, 72, 70)
bmi <- wiight / (height / 100)^2
bmi <- weight / (height / 100)^2
bmi
lenght(bmi)
length(bmi)
x <- c(1, -2, 3, -4, 5, -6, 7, -8)
x[x > 0]
x[x > 0] * 10
x <- c(1,2,3)
x[1]/x[2]^2-1+2*x[3]-x[1+1]
x <- 1:200
length(x[x %% 11 == 0])
q()
gcd(5,8)
q()
vehicle <- read.table("vehicle.txt", sep=",", header = T)
getwd()
vehicle <- read.table("vehicle.txt", sep=",", header = T)
summary(vehicle)
set.seed(8678686)
sel <- sample(1:nrow(vehicle), size=as.integer(nrow(vehicle)*0.7), replace=F)
learn <- vehicle[sel,]
test <- vehicle[-sel,]
table(learn$Class)
table(test$Class)
library(CORElearn)
modelDT <- CoreModel(Class ~ ., learn, model="tree")
modelNB <- CoreModel(Class ~ ., learn, model="bayes")
modelKNN <- CoreModel(Class ~ ., learn, model="knn", kInNN = 5)
predDT <- predict(modelDT, test, type = "class")
caDT <- CA(test$Class, predDT)
source("mojefunkcije.R")
modelDT <- CoreModel(Class ~ ., learn, model="tree")
modelNB <- CoreModel(Class ~ ., learn, model="bayes")
modelKNN <- CoreModel(Class ~ ., learn, model="knn", kInNN = 5)
predDT <- predict(modelDT, test, type = "class")
caDT <- CA(test$Class, predDT)
caDT
predNB <- predict(modelNB, test, type="class")
caNB <- CA(test$Class, predNB)
caNB
predKNN <- predict(modelKNN, test, type="class")
caKNN <- CA(test$Class, predKNN)
caKNN
pred <- data.frame(predDT, predNB, predKNN)
pred
voting <- function(predictions)
{
res <- vector()
  for (i in 1 : nrow(predictions))  
{
vec <- unlist(predictions[i,])
    res[i] <- names(which.max(table(vec)))
  }
  factor(res, levels=levels(predictions[,1]))
}
predicted <- voting(pred)
CA(test$Class, predicted)
predDT.prob <- predict(modelDT, test, type="probability")
predNB.prob <- predict(modelNB, test, type="probability")
predKNN.prob <- predict(modelKNN, test, type="probability")
# sestejemo napovedane verjetnosti s strani razlicnih modelov
pred.prob <- caDT * predDT.prob + caNB * predNB.prob + caKNN * predKNN.prob
pred.prob
# izberemo razred z najvecjo verjetnostjo
predicted <- levels(learn$Class)[apply(pred.prob, 1, which.max)]
CA(test$Class, predicted)
#install.packages("ipred")
library(ipred)
install.packages("ipred")
library(ipred)
bag <- bagging(Class ~ ., learn, nbagg=15)
bag.pred <- predict(bag, test, type="class")
CA(test$Class, bag.pred)
bag <- bagging(Class ~ ., learn, nbagg=15)
bag.pred <- predict(bag, test, type="class")
CA(test$Class, bag.pred)
bag <- bagging(Class ~ ., learn, nbagg=15)
bag.pred <- predict(bag, test, type="class")
CA(test$Class, bag.pred)
bag <- bagging(Class ~ ., learn, nbagg=15)
bag.pred <- predict(bag, test, type="class")
CA(test$Class, bag.pred)
bag <- bagging(Class ~ ., learn, nbagg=15)
bag.pred <- predict(bag, test, type="class")
CA(test$Class, bag.pred)
bag <- bagging(Class ~ ., learn, nbagg=15)
bag.pred <- predict(bag, test, type="class")
CA(test$Class, bag.pred)
bag <- bagging(Class ~ ., learn, nbagg=15)
bag.pred <- predict(bag, test, type="class")
CA(test$Class, bag.pred)
bag <- bagging(Class ~ ., learn, nbagg=15)
bag.pred <- predict(bag, test, type="class")
CA(test$Class, bag.pred)
bag <- bagging(Class ~ ., learn, nbagg=15)
bag.pred <- predict(bag, test, type="class")
CA(test$Class, bag.pred)
bag <- bagging(Class ~ ., learn, nbagg=15)
bag.pred <- predict(bag, test, type="class")
CA(test$Class, bag.pred)
library(randomForest)
# install.packages("randomForest")
install.packages("randomForest")
#
library(randomForest)
rf <- randomForest(Class ~ ., learn)
predicted <- predict(rf, test, type = "class")
CA(test$Class, predicted)
rf <- randomForest(Class ~ ., learn)
predicted <- predict(rf, test, type = "class")
CA(test$Class, predicted)
rf <- randomForest(Class ~ ., learn)
predicted <- predict(rf, test, type = "class")
CA(test$Class, predicted)
rf <- randomForest(Class ~ ., learn)
predicted <- predict(rf, test, type = "class")
CA(test$Class, predicted)
rf <- randomForest(Class ~ ., learn)
predicted <- predict(rf, test, type = "class")
CA(test$Class, predicted)
rf <- randomForest(Class ~ ., learn)
predicted <- predict(rf, test, type = "class")
CA(test$Class, predicted)
rf <- randomForest(Class ~ ., learn)
predicted <- predict(rf, test, type = "class")
CA(test$Class, predicted)
library(adabag)
# install.packages("adabag")
install.packages("adabag")
library(adabag)
bm <- boosting(Class ~ ., learn)
predictions <- predict(bm, test)
predictions <- predict(bm, test)
names(predictions)
predicted <- predictions$class
CA(test$Class, predicted)
library(CORElearn)
mr <- read.table("mushroom.txt", sep=",", header=T)
summary(mr)
summary(mr)
barplot(table(mr$edibility), ylab="Number of species", main="Edibility of mushrooms")
sort(attrEval(edibility ~ ., mr, "InfGain"), decreasing = TRUE)
dt <- CoreModel(edibility ~ ., mr, model="tree", selectionEstimator="InfGain")
plot(dt, mr)
quadrant <- read.table("quadrant.txt", sep=",", header=T)
summary(quadrant)
quadrant$Class <- as.factor(quadrant$Class)
summary(quadrant)
plot(quadrant, col=quadrant$Class)
plot(quadrant$a1, quadrant$a2, col=quadrant$Class)
sort(attrEval(Class ~ ., quadrant, "InfGain"), decreasing = TRUE)
dt2 <- CoreModel(Class ~ ., quadrant, model="tree", selectionEstimator="InfGain")
plot(dt2, quadrant)
sort(attrEval(Class ~ ., quadrant, "InfGain"), decreasing = TRUE)
sort(attrEval(Class ~ ., quadrant, "Gini"), decreasing = TRUE)
sort(attrEval(Class ~ ., quadrant, "MDL"), decreasing = TRUE)
sort(attrEval(Class ~ ., quadrant, "Relief"), decreasing = TRUE)
sort(attrEval(Class ~ ., quadrant, "ReliefFequalK"), decreasing = TRUE)
sort(attrEval(Class ~ ., quadrant, "ReliefFexpRank"), decreasing = TRUE)
dt3 <- CoreModel(Class ~ ., quadrant, model="tree", selectionEstimator = "ReliefFequalK")
plot(dt3, quadrant)
players <- read.table("players.txt", sep=",", header=TRUE)
summary(players)
sort(attrEval(position ~ ., players, "InfGain"), decreasing = TRUE)
sort(attrEval(position ~ ., players, "Gini"), decreasing = TRUE)
# Atribut "id" je precenjen, ceprav ne nosi nobene koristne informacije za 
# napovedovanje novih primerov
# GainRatio omili precenjevanje atributa "id"
sort(attrEval(position ~ ., players, "GainRatio"), decreasing = TRUE)
# ReliefF in MDL pravilno ocenita "id" kot nepomemben atribut
sort(attrEval(position ~ ., players, "ReliefFequalK"), decreasing = TRUE)
sort(attrEval(position ~ ., players, "MDL"), decreasing = TRUE)
#
#
# Izbira podmnozice atributov
#
#
student <- read.table("student.txt", sep=",", header=T)
student$G1 <- cut(student$G1, c(-Inf, 9, 20), labels=c("fail", "pass"))
student$G2 <- cut(student$G2, c(-Inf, 9, 20), labels=c("fail", "pass"))
student$G3 <- cut(student$G3, c(-Inf, 9, 20), labels=c("fail", "pass"))
student$studytime <- cut(student$studytime, c(-Inf, 1, 2, 3, 4), labels=c("none", "few", "hefty", "endless"))
library(ipred)
mymodel <- function(formula, data, target.model){CoreModel(formula, data, model=target.model)}
mypredict <- function(object, newdata) {pred <- predict(object, newdata)$class; destroyModels(object); pred}
res <- errorest(studytime ~ ., data = student, model = mymodel, predict = mypredict, target.model="tree")
1-res$error
sort(attrEval(studytime ~ ., student, "ReliefFequalK"), decreasing = TRUE)
res <- errorest(studytime ~ sex + G1 + famsup + reason + romantic + G2, data = student, model = mymodel, predict = mypredict, target.model="tree")
1-res$error
# S pravo izbiro lahko presezemo tocnost modela, ki se je ucil iz celotnega nabora atributov
res <- errorest(studytime ~ Dalc + paid + G2, data = student, model = mymodel, predict = mypredict, target.model="tree")
1-res$error
source("wrapper.R")
wrapper(student, className="studytime", classModel="tree", folds=10)
q()
